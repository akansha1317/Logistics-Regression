{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Theoretical***"
      ],
      "metadata": {
        "id": "MQLWgf9s4Ozt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "- Logistic regression is a statistical method used for binary classification problems, where the outcome or dependent variable is categorical, typically representing two classes such as 0 and 1. It models the probability that a given input belongs to a particular class by applying the logistic (sigmoid) function to a linear combination of input features. This function transforms any real-valued number into a value between 0 and 1, making it suitable for probability estimation. In contrast, linear regression is used for regression problems, where the dependent variable is continuous. It assumes a linear relationship between the input variables and the output, predicting values by fitting a straight line that minimizes the difference between predicted and actual values using the least squares method. The key theoretical difference is that logistic regression focuses on estimating probabilities for classification using maximum likelihood estimation, while linear regression aims to predict continuous outcomes based on a linear approximation using least squares.\n"
      ],
      "metadata": {
        "id": "922CprK_4Yj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "- The mathematical equation of logistic regression models the probability that a given input belongs to a particular class, typically class 1 in a binary classification problem. It begins by calculating a linear combination of the input features, expressed as $z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n$, where $\\beta_0$ is the intercept, $\\beta_1, \\ldots, \\beta_n$ are the coefficients, and $x_1, \\ldots, x_n$ are the input variables. This linear result is then passed through the sigmoid function, which transforms the value into a range between 0 and 1, making it interpretable as a probability. The final equation becomes $P(Y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-z}}$, which estimates the probability that the dependent variable $Y$ equals 1 given the input $\\mathbf{x}$. This probabilistic output can be thresholded (e.g., at 0.5) to make a binary classification decision.\n"
      ],
      "metadata": {
        "id": "UFS_3mcP43dU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "\n",
        "- We use the **sigmoid function** in logistic regression because it transforms any real-valued input into a value between **0 and 1**, making it ideal for modeling **probabilities**. In classification problems, particularly binary classification, we want to predict the likelihood that a given input belongs to a particular class (e.g., class 1). The raw output from the linear combination of features in logistic regression can range from negative to positive infinity, which isn't suitable for interpreting as a probability. The sigmoid function solves this by \"squashing\" the output into a bounded interval, allowing us to interpret the result as the **probability of the positive class**. Moreover, the sigmoid function has nice mathematical properties—it's smooth, differentiable, and has a well-defined gradient—which make it suitable for optimization using methods like **gradient descent**. In summary, the sigmoid function is used because it effectively maps the model’s output to a probability, enabling logistic regression to function as a classification tool.\n"
      ],
      "metadata": {
        "id": "eF5J49SR5MKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression.\n",
        "\n",
        "- The cost function used in logistic regression is known as the **log loss** or **binary cross-entropy** function. It is designed to evaluate how well the predicted probabilities align with the actual binary class labels (0 or 1). Since logistic regression outputs probabilities rather than direct class labels, this cost function penalizes incorrect predictions based on how far the predicted probability is from the actual class. Mathematically, for each training example, the cost is defined as $-[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})]$, where $y$ is the true label and $\\hat{y}$ is the predicted probability of class 1. The total cost over all training examples is the average of these individual costs. This function is preferred because it is convex, ensuring a single global minimum that optimization algorithms can efficiently find, and it is derived from the principle of maximum likelihood estimation. The log loss cost function effectively guides the logistic regression model in learning accurate probabilities and making better classification decisions.\n"
      ],
      "metadata": {
        "id": "Q5yFcvWY5VGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "\n",
        "- **Regularization** in logistic regression is a technique used to **prevent overfitting** by adding a **penalty term** to the cost function. In a standard logistic regression model, the algorithm tries to find the set of coefficients (weights) that best fit the training data. However, if the model is too complex (for example, if it has too many features or highly flexible coefficients), it may fit the training data too closely, including noise, and perform poorly on new, unseen data. This is where regularization helps.\n",
        "\n",
        "The idea is to discourage the model from assigning excessively large weights to any feature by adding a penalty to the cost function based on the size of the coefficients. The two most common types of regularization are **L1 regularization (Lasso)**, which adds the sum of the absolute values of the coefficients, and **L2 regularization (Ridge)**, which adds the sum of the squares of the coefficients. In logistic regression, the regularized cost function with L2 regularization looks like this:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "Here, $\\lambda$ is the regularization parameter that controls the strength of the penalty. A larger $\\lambda$ increases the penalty, reducing model complexity, while a smaller $\\lambda$ allows more flexibility. Regularization is needed to improve the model’s generalization performance and ensure it doesn’t memorize the training data, which is especially important when dealing with high-dimensional data or features that are highly correlated.\n"
      ],
      "metadata": {
        "id": "eamheukW5-R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "- Lasso, Ridge, and Elastic Net are regularization techniques used to prevent overfitting by adding penalties to the regression model’s coefficients, but they differ in how these penalties are applied. Ridge regression uses an L2 penalty, which adds the sum of the squared coefficients to the loss function. This approach shrinks coefficients toward zero but does not set any exactly to zero, so all features remain in the model. Lasso regression, on the other hand, uses an L1 penalty by adding the sum of the absolute values of the coefficients. This encourages sparsity by driving some coefficients exactly to zero, effectively performing feature selection by removing less important features. Elastic Net combines both L1 and L2 penalties, balancing the benefits of Ridge’s coefficient shrinkage and Lasso’s feature selection, making it particularly useful when dealing with many correlated features. Overall, Ridge is preferred when all features contribute to the outcome, Lasso is useful when only a few features matter, and Elastic Net is a compromise that handles correlated features better than Lasso alone.\n"
      ],
      "metadata": {
        "id": "Yx8uVSxQ67jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "- Elastic Net should be used instead of Lasso or Ridge when you have a dataset with **many correlated features** or when you suspect that a combination of both feature selection and coefficient shrinkage is needed. While Lasso tends to select only one feature from a group of correlated variables and ignore the others, Elastic Net can retain groups of correlated features together because it combines both L1 (which encourages sparsity) and L2 (which encourages grouping) penalties. This makes Elastic Net especially effective in situations where features are highly correlated or when the number of features exceeds the number of observations. It balances the strengths of Ridge and Lasso, providing better predictive performance and more stable feature selection in complex, high-dimensional data scenarios.\n"
      ],
      "metadata": {
        "id": "sek7WBz475-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "\n",
        "- The regularization parameter $\\lambda$ in logistic regression controls the **strength of the penalty** applied to the model’s coefficients to prevent overfitting. A **larger $\\lambda$** increases the penalty, forcing the coefficients to shrink closer to zero, which simplifies the model and reduces variance but may increase bias, potentially underfitting the data. Conversely, a **smaller $\\lambda$** applies less penalty, allowing the coefficients to take larger values, which can fit the training data more closely but risks overfitting and poor generalization to new data. Essentially, $\\lambda$ balances the trade-off between bias and variance: tuning it correctly helps the model generalize better by controlling complexity, while an inappropriate value can lead to either an overly simplistic or overly complex model.\n"
      ],
      "metadata": {
        "id": "Z1R1n12A8EkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "\n",
        "- The key assumptions of logistic regression are:\n",
        "\n",
        "1. **Linearity of the Logit**: Logistic regression assumes a linear relationship between the independent variables and the **log-odds** (logit) of the dependent binary outcome. This means that the predictors combine linearly to predict the logit of the probability.\n",
        "\n",
        "2. **Independent Observations**: The observations should be independent of each other; there should be no correlation or dependence between the data points.\n",
        "\n",
        "3. **No Perfect Multicollinearity**: The independent variables should not be perfectly correlated with each other. High multicollinearity can distort the estimates and make it difficult to determine the individual effect of each predictor.\n",
        "\n",
        "4. **Large Sample Size**: Logistic regression requires a sufficiently large sample size to provide reliable estimates, especially when dealing with multiple predictors.\n",
        "\n",
        "5. **Appropriate Outcome Variable**: The dependent variable should be binary (or can be extended to multinomial logistic regression for multiple categories), representing distinct classes.\n",
        "\n",
        "Unlike linear regression, logistic regression does **not** assume homoscedasticity (constant variance) or normally distributed residuals, making it more flexible for classification tasks.\n"
      ],
      "metadata": {
        "id": "FUKzMR9a8apx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        "- Some common alternatives to logistic regression for classification tasks include **Decision Trees**, **Random Forests**, **Support Vector Machines (SVMs)**, **K-Nearest Neighbors (KNN)**, **Naive Bayes**, and **Neural Networks**. Decision Trees split the data into branches based on feature values, making them easy to interpret. Random Forests build multiple trees and aggregate their results for better accuracy and robustness. SVMs find the optimal boundary (hyperplane) that separates classes with maximum margin, which can work well with high-dimensional data. KNN classifies data points based on the majority class among their closest neighbors, relying on distance metrics. Naive Bayes applies Bayes’ theorem with an assumption of feature independence, often performing well on text classification. Neural Networks, especially deep learning models, can capture complex nonlinear relationships and are widely used in image, speech, and language tasks. The choice among these depends on factors like dataset size, feature characteristics, interpretability, and computational resources.\n"
      ],
      "metadata": {
        "id": "kUXYhVqM8kYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics.\n",
        "\n",
        "- **Classification evaluation metrics** are measures used to assess how well a classification model performs in predicting the correct class labels. Common metrics include:\n",
        "\n",
        "* **Accuracy**: The proportion of total correct predictions out of all predictions made. It’s simple but can be misleading if classes are imbalanced.\n",
        "\n",
        "* **Precision**: The proportion of correctly predicted positive instances out of all instances predicted as positive. It answers, \"Of all predicted positives, how many were actually positive?\"\n",
        "\n",
        "* **Recall (Sensitivity or True Positive Rate)**: The proportion of correctly predicted positive instances out of all actual positives. It answers, \"Of all actual positives, how many did the model correctly identify?\"\n",
        "\n",
        "* **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two, especially useful when classes are imbalanced.\n",
        "\n",
        "* **Specificity (True Negative Rate)**: The proportion of correctly predicted negative instances out of all actual negatives.\n",
        "\n",
        "* **ROC Curve and AUC (Area Under the Curve)**: The ROC curve plots the true positive rate against the false positive rate at different threshold levels. The AUC quantifies overall model performance; closer to 1 means better discrimination.\n",
        "\n",
        "* **Confusion Matrix**: A table showing counts of true positives, true negatives, false positives, and false negatives, giving a detailed view of classification errors.\n",
        "\n",
        "These metrics help evaluate the effectiveness of a classifier and guide model selection and tuning.\n"
      ],
      "metadata": {
        "id": "T0d_Eht78rEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How does class imbalance affect Logistic Regression.\n",
        "\n",
        "- Class imbalance occurs when one class significantly outnumbers the other(s) in a classification problem, and it can negatively affect logistic regression in several ways. Because logistic regression aims to maximize overall accuracy or likelihood, it tends to be biased toward the majority class, often predicting the dominant class more frequently while neglecting the minority class. This can lead to poor detection of the minority class, which might be the more important class in many applications (like fraud detection or disease diagnosis). As a result, metrics like accuracy can be misleadingly high despite poor performance on the minority class. Additionally, the estimated probabilities and model coefficients can become skewed, reducing the model’s ability to generalize well. To address this, techniques such as resampling (oversampling the minority or undersampling the majority), using different evaluation metrics (like F1 score or AUC), or applying specialized algorithms or regularization methods are often employed.\n"
      ],
      "metadata": {
        "id": "avVhCcgE80vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        "- Hyperparameter tuning in logistic regression refers to the process of selecting the best set of **hyperparameters**—parameters that are not learned from the data but set before training—to optimize the model’s performance. In logistic regression, key hyperparameters include the **regularization strength (λ)**, which controls how much penalty is applied to the coefficients to prevent overfitting, and the **type of regularization** used (such as L1, L2, or Elastic Net). Tuning these hyperparameters involves systematically searching through possible values, often using techniques like **grid search** or **random search**, combined with cross-validation to evaluate how well the model generalizes to unseen data. The goal is to find the hyperparameter values that minimize error or maximize a chosen evaluation metric, ensuring the model balances bias and variance effectively for better predictive accuracy.\n"
      ],
      "metadata": {
        "id": "yMrLhz8788zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "\n",
        "- In logistic regression, solvers are optimization algorithms used to find the best model parameters by minimizing the cost function, and different solvers are suited to different types of data and regularization. Common solvers include **liblinear**, which is efficient for small to medium-sized datasets and supports both L1 and L2 regularization; **lbfgs**, a quasi-Newton method that works well for medium to large datasets but supports only L2 regularization; and **saga**, which is designed for very large or sparse datasets and supports Elastic Net as well as L1 and L2 regularization. Gradient descent is another method but is less commonly used in practice due to slower convergence. Generally, **liblinear** is preferred for smaller datasets or when L1 regularization is needed, **lbfgs** is often the default choice for larger datasets with L2 regularization, and **saga** is best for large-scale or sparse data and when using Elastic Net. The choice of solver depends on factors like dataset size, the type of regularization, and computational efficiency.\n"
      ],
      "metadata": {
        "id": "iNTW6aJ-9JBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How is Logistic Regression extended for multiclass classification.\n",
        "\n",
        "- Logistic regression is extended to multiclass classification problems using approaches that allow it to handle more than two classes, since standard logistic regression is inherently binary. The two most common methods are **One-vs-Rest (OvR)** and **Softmax Regression (also called Multinomial Logistic Regression)**. In the OvR approach, the model trains one binary classifier per class, where each classifier distinguishes that class from all others; during prediction, the class whose classifier outputs the highest probability is chosen. Softmax regression, on the other hand, generalizes logistic regression by modeling the probabilities of each class simultaneously using the softmax function, which converts raw scores (logits) for all classes into probabilities that sum to one. Softmax regression directly optimizes a likelihood function for multiple classes and often performs better when classes are mutually exclusive. These extensions enable logistic regression to be applied effectively to problems involving multiple categories.\n"
      ],
      "metadata": {
        "id": "kJnykqkQ9TVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "\n",
        "- Logistic regression offers several advantages and disadvantages as a classification method.\n",
        "\n",
        "**Advantages** include its simplicity and ease of implementation, making it a good baseline model. It provides **probabilistic outputs**, which are interpretable as the likelihood of class membership, and the model coefficients offer clear insights into the relationship between features and the outcome. Logistic regression is also **computationally efficient** and works well when the relationship between the features and the log-odds of the outcome is approximately linear. Additionally, it can be regularized to prevent overfitting and handle high-dimensional data.\n",
        "\n",
        "**Disadvantages** include its assumption of a linear relationship between the input features and the log-odds, which may not hold true in complex real-world problems, limiting its ability to capture nonlinear patterns. It is also sensitive to **outliers** and may perform poorly when classes are not linearly separable. Logistic regression can struggle with **multicollinearity** among features, and its performance can degrade when the dataset is imbalanced. For very large-scale or highly complex tasks, more flexible models like decision trees, random forests, or neural networks may be more effective.\n"
      ],
      "metadata": {
        "id": "GsaV1O909dfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression.\n",
        "\n",
        "- Logistic regression is widely used across various fields for binary and multiclass classification tasks due to its simplicity and interpretability. Common use cases include **medical diagnosis**, where it helps predict the presence or absence of diseases based on patient data; **credit scoring** in finance, assessing the likelihood of loan default; **email spam detection**, classifying messages as spam or not; and **customer churn prediction**, identifying customers likely to leave a service. It’s also used in **marketing** to predict whether a user will respond to a campaign or purchase a product, and in **social sciences** for modeling binary outcomes like voting behavior or survey responses. Because logistic regression outputs probabilities, it is especially useful in applications requiring clear decision thresholds or risk assessments.\n"
      ],
      "metadata": {
        "id": "ebzg5nis9nD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "\n",
        "- Logistic regression and softmax regression are both used for classification, but they differ primarily in the number of classes they handle and how they model probabilities. Logistic regression is designed for **binary classification** problems, predicting the probability that an input belongs to one of two classes using the sigmoid function, which maps the output to a value between 0 and 1. In contrast, softmax regression (also called multinomial logistic regression) is an extension of logistic regression used for **multiclass classification** problems, where there are three or more classes. Instead of the sigmoid, softmax regression uses the **softmax function** to convert raw scores (logits) for each class into probabilities that sum to one across all classes. While logistic regression models the log-odds of a single class versus the other, softmax regression models the probability distribution over multiple classes simultaneously, making it suitable for multiclass tasks.\n"
      ],
      "metadata": {
        "id": "gUKZ9_L4-2a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "\n",
        "- Choosing between One-vs-Rest (OvR) and Softmax (multinomial logistic regression) for multiclass classification depends on factors like dataset size, class distribution, and computational resources. OvR is simpler and often faster, as it breaks the multiclass problem into multiple independent binary classifiers, which can be easier to implement and parallelize. It can work well when classes are not strongly overlapping and when interpretability of individual binary classifiers is desired. However, OvR may suffer if classes are highly imbalanced or overlapping, since each classifier ignores the relationships among other classes. Softmax, on the other hand, models all classes simultaneously, producing a probability distribution across all classes, which often leads to better performance when classes are mutually exclusive and well-defined. Softmax regression tends to be more accurate in capturing interactions among classes but requires more computation and memory. In summary, OvR can be preferred for simplicity and scalability on large or imbalanced datasets, while Softmax is favored when class relationships are important and more precise probability estimates are needed.\n"
      ],
      "metadata": {
        "id": "e6AoJbBn_L1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "- In logistic regression, the coefficients represent the relationship between each feature and the **log-odds** of the outcome occurring (i.e., the event being in class 1). Specifically, a coefficient tells us how much the log-odds of the dependent variable increase or decrease with a one-unit increase in the corresponding feature, assuming all other variables are held constant. A **positive coefficient** means that as the feature increases, the likelihood of the event occurring (class 1) increases, while a **negative coefficient** indicates a decrease in that likelihood. To make interpretation more intuitive, coefficients are often exponentiated to obtain **odds ratios**: for example, an odds ratio of 2 means that a one-unit increase in the feature doubles the odds of the event occurring. This makes logistic regression particularly useful for understanding and quantifying the impact of predictors on binary outcomes.\n"
      ],
      "metadata": {
        "id": "uYKJHjGd_T45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Practical***"
      ],
      "metadata": {
        "id": "iYVKPPSr_a6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "nhsrKO9j_fZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbznyQSr_lWB",
        "outputId": "5d5c33bf-1d9a-40ce-e766-2547ecebc344"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ],
      "metadata": {
        "id": "Pew1WZNB_vKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=500)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with L1 regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft5Pzz-D_1-R",
        "outputId": "2aa4b0ff-432d-4a46-c26e-06238309668c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L1 regularization: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "5uv5kM6dAAjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=500, multi_class='auto')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with L2 regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Model coefficients:\")\n",
        "for i, class_label in enumerate(data.target_names):\n",
        "    coef = model.coef_[i]\n",
        "    print(f\"Class '{class_label}': {coef}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oYZ0FnxAD7Y",
        "outputId": "e4a18b6d-2287-4d64-a0ee-1362386d06b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with L2 regularization: 1.00\n",
            "Model coefficients:\n",
            "Class 'setosa': [-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            "Class 'versicolor': [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            "Class 'virginica': [-0.11497673 -0.70769055  2.58813565  1.7744936 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n"
      ],
      "metadata": {
        "id": "E854NvmaBjdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,      # balance between L1 and L2 (0 = Ridge, 1 = Lasso)\n",
        "    max_iter=1000,\n",
        "    multi_class='auto',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with Elastic Net regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEO-fVRXB3Kw",
        "outputId": "79ff98fe-5f7d-44ff-f09e-5ad09c676277"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with Elastic Net regularization: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "zMticNVnB_M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model with One-vs-Rest multiclass strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=500)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy with OvR strategy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s85nnvLcCCIn",
        "outputId": "7b7e9ea7-f2c8-4fa9-a1ad-798eb2d81b45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with OvR strategy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "rzh6I_YgCNeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the logistic regression model (solver 'saga' supports both L1 and L2 penalties)\n",
        "model = LogisticRegression(solver='saga', max_iter=500, multi_class='auto')\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],           # inverse regularization strength\n",
        "    'penalty': ['l1', 'l2']                  # regularization types\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearch to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score from GridSearch\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate best estimator on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toNfK_KKCQn3",
        "outputId": "9d296361-dddd-4e26-9379-b2cfa3bbfba8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 1, 'penalty': 'l1'}\n",
            "Best cross-validation accuracy: 0.97\n",
            "Test set accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "YVunKSN_CYUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize logistic regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs', multi_class='auto')\n",
        "\n",
        "# Define Stratified K-Fold cross-validator with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Evaluate model using cross-validation and get accuracy scores\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print accuracy for each fold and average accuracy\n",
        "for i, score in enumerate(scores, 1):\n",
        "    print(f\"Fold {i} accuracy: {score:.2f}\")\n",
        "print(f\"Average accuracy: {scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpcvLPFHCbkx",
        "outputId": "dc187a4b-8860-42d1-8910-d82f84d87f7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 accuracy: 1.00\n",
            "Fold 2 accuracy: 0.97\n",
            "Fold 3 accuracy: 0.93\n",
            "Fold 4 accuracy: 1.00\n",
            "Fold 5 accuracy: 0.93\n",
            "Average accuracy: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "h2cJwj4yCjwI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoJsRsI9DP_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "fzRZoL7fDQrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, multi_class='auto')\n",
        "\n",
        "# Define hyperparameter space for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),                   # Continuous uniform distribution between 0.01 and 10\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],   # Penalties to try\n",
        "    'solver': ['saga', 'liblinear'],          # solvers compatible with penalties\n",
        "    'l1_ratio': [None, 0.5, 0.7]              # l1_ratio used only if penalty='elasticnet'\n",
        "}\n",
        "\n",
        "# Custom function to handle incompatible solver-penalty combos by ignoring those params in search\n",
        "def is_valid_combination(params):\n",
        "    penalty = params['penalty']\n",
        "    solver = params['solver']\n",
        "    # 'liblinear' supports only l1 and l2, not elasticnet\n",
        "    if penalty == 'elasticnet' and solver != 'saga':\n",
        "        return False\n",
        "    # 'liblinear' does not support l1_ratio\n",
        "    if solver == 'liblinear' and params.get('l1_ratio', None) is not None:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Filter param_dist space by defining a custom scoring and refitting function\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "\n",
        "# Generate all sampled params\n",
        "n_iter_search = 20\n",
        "param_list = list(ParameterSampler(param_dist, n_iter=n_iter_search, random_state=42))\n",
        "# Filter invalid combos\n",
        "param_list = [p for p in param_list if is_valid_combination(p)]\n",
        "\n",
        "# Run RandomizedSearchCV with filtered params\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=n_iter_search,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best params\n",
        "print(\"Best parameters found:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate best model on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dPH1LRIDUnP",
        "outputId": "fe876ba4-e77c-493a-a8df-3801c5ececbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'C': np.float64(3.347086111390218), 'l1_ratio': 0.7, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Test set accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "30 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.96666667 0.95833333 0.95\n",
            " 0.95833333 0.95833333 0.95              nan 0.95833333        nan\n",
            " 0.95833333 0.95       0.95       0.91666667 0.94166667 0.96666667\n",
            " 0.95833333        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "Yalrg0paDnc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "\n",
        "# Wrap Logistic Regression with One-vs-One strategy\n",
        "ovo_clf = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = ovo_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLlXYoG7Dq9P",
        "outputId": "e4220f4b-1054-43a3-d0c7-df4b675d2908"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification.\n"
      ],
      "metadata": {
        "id": "Q6uCO4wWDynH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load binary classification dataset (Breast Cancer)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "MUVxcTwZD1mQ",
        "outputId": "f4a549fe-7ed3-4b9a-fa5d-34d4cad4f9f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "x6Wg5rLRD8ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOsFWZpqD_kw",
        "outputId": "975f1cc7-b15c-4f2b-8155-1e7ecb8e3205"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.95\n",
            "Recall: 0.99\n",
            "F1-Score: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "0elT1ED9EJgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "    n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42\n",
        ")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Train Logistic Regression with class weights balanced\n",
        "model_with_weights = LogisticRegression(max_iter=500, solver='lbfgs', class_weight='balanced')\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "print(\"With class weights balanced:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2kKSnspERBP",
        "outputId": "c67f670c-692f-4d32-b70f-708946bc902f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       186\n",
            "           1       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.99       200\n",
            "   macro avg       0.97      1.00      0.98       200\n",
            "weighted avg       1.00      0.99      1.00       200\n",
            "\n",
            "With class weights balanced:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98       186\n",
            "           1       0.67      1.00      0.80        14\n",
            "\n",
            "    accuracy                           0.96       200\n",
            "   macro avg       0.83      0.98      0.89       200\n",
            "weighted avg       0.98      0.96      0.97       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "UAYma7GEEXsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load Titanic dataset (replace with your CSV path if needed)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Select features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = data[features]\n",
        "y = data['Survived']\n",
        "\n",
        "# Define preprocessing for numeric features: impute missing values with mean\n",
        "numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "numeric_transformer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Define preprocessing for categorical features: impute missing with most frequent and one-hot encode\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create pipeline with preprocessing and logistic regression\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression accuracy on Titanic dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUv1eNEcEbHH",
        "outputId": "dd67a0e6-5499-4957-ed3f-78404fa11a81"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy on Titanic dataset: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "Mr3n0TThEj1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Without scaling ---\n",
        "model_no_scaling = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# --- With standardization ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with standardization: {acc_scaling:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KryeLu8uEpDf",
        "outputId": "9d298a11-42b5-46c5-fcd5-c4c6993aeb67"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.96\n",
            "Accuracy with standardization: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n"
      ],
      "metadata": {
        "id": "3fxVQU8DEvx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f\"ROC-AUC score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbpyw17GE0Yf",
        "outputId": "d87c7be7-3fa8-4ba4-aecb-a34d42d2db97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy.\n"
      ],
      "metadata": {
        "id": "Pioq9YndHNWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with custom regularization strength C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=500, solver='lbfgs')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKd2h1H2H0EW",
        "outputId": "ad16f81d-d74a-48ad-a0b9-06145546b741"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy with C=0.5: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients.\n"
      ],
      "metadata": {
        "id": "wkRvuD-zIHhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame for easy viewing\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Abs_Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by absolute value of coefficients\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Print top important features\n",
        "print(\"Top important features based on logistic regression coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCKREMDEIO9d",
        "outputId": "9ffcf6db-036e-4534-fd77-e3cff32bb80b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top important features based on logistic regression coefficients:\n",
            "                 Feature  Coefficient\n",
            "0            mean radius     2.132484\n",
            "26       worst concavity    -1.617969\n",
            "11         texture error     1.442984\n",
            "20          worst radius     1.232150\n",
            "25     worst compactness    -1.208985\n",
            "28        worst symmetry    -0.742764\n",
            "6         mean concavity    -0.651940\n",
            "27  worst concave points    -0.615251\n",
            "5       mean compactness    -0.415569\n",
            "21         worst texture    -0.404581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "fRB4-T9TIXiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen’s Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print result\n",
        "print(f\"Cohen’s Kappa Score: {kappa_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcJM8S2bIbrN",
        "outputId": "f3d9f0c0-09d2-45cf-ee2e-049cf2d5df00"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen’s Kappa Score: 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "3Z_wfGq1Iogl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision and recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "jczslAy0I0_G",
        "outputId": "6d43723b-e4b0-43a9-f9f7-592dfe0d0372"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "9NFDnTNNJVS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression with different solvers\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=500)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        accuracies[solver] = acc\n",
        "    except Exception as e:\n",
        "        accuracies[solver] = f\"Error: {e}\"\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy comparison of Logistic Regression with different solvers:\")\n",
        "for solver, acc in accuracies.items():\n",
        "    print(f\"{solver}: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhkGk_RcJbT1",
        "outputId": "d46f275d-3e3a-4bb5-cd40-bf435a7be5a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy comparison of Logistic Regression with different solvers:\n",
            "liblinear: 0.956140350877193\n",
            "saga: 0.956140350877193\n",
            "lbfgs: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "KpMEgWeJJi19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print MCC\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je0t7RitJl_k",
        "outputId": "bd81dad3-8a5b-40c1-ec37-c1bf2884f4d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "XCLLnoo9Juy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Logistic Regression on raw data ---\n",
        "model_raw = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# --- Logistic Regression on standardized data ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --- Compare results ---\n",
        "print(f\"Accuracy on raw data:         {accuracy_raw:.2f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MArgeWztJ0j7",
        "outputId": "27fa87f2-ca9c-4523-a414-d25954b442f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data:         0.96\n",
            "Accuracy on standardized data: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "D2M7_ABmKOyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500, solver='liblinear')\n",
        "\n",
        "# Define the range of C values to test\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Use GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best C value\n",
        "best_c = grid_search.best_params_['C']\n",
        "print(f\"Best C value from cross-validation: {best_c}\")\n",
        "\n",
        "# Train final model with best C\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy with best C: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue9Ygwb_KSdF",
        "outputId": "682f6221-9a56-40c4-a496-0fb206421278"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value from cross-validation: 10\n",
            "Test accuracy with best C: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions.\n"
      ],
      "metadata": {
        "id": "u_sEGpoUKdvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "print(\"Model saved as 'logistic_model.joblib'\")\n",
        "\n",
        "# Step 5: Load the model from disk\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Step 6: Predict using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy from loaded model: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfLrCNKkKhrE",
        "outputId": "d228a946-b162-4b81-f196-62e16c4e8109"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'logistic_model.joblib'\n",
            "Model loaded successfully.\n",
            "Accuracy from loaded model: 0.96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}